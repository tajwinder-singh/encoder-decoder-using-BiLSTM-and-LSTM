{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-15T07:44:10.272187Z",
     "iopub.status.busy": "2025-10-15T07:44:10.271495Z",
     "iopub.status.idle": "2025-10-15T07:44:35.129019Z",
     "shell.execute_reply": "2025-10-15T07:44:35.128225Z",
     "shell.execute_reply.started": "2025-10-15T07:44:10.272164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 07:44:13.617164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760514253.793614      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760514253.854895      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr_core_news_sm\n",
      "  Downloading fr_core_news_sm-3.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr_core_news_sm\n",
      "Successfully installed fr_core_news_sm-3.8.0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "import sentencepiece as spm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "!pip install fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:14:04.021287Z",
     "iopub.status.busy": "2025-10-14T13:14:04.020681Z",
     "iopub.status.idle": "2025-10-14T13:14:08.532171Z",
     "shell.execute_reply": "2025-10-14T13:14:08.531570Z",
     "shell.execute_reply.started": "2025-10-14T13:14:04.021259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdda090667db444cae9d41a757107799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7f037bff584170a462c94c2f696fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4874068372b472a864aa49fc1d208ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:14:08.533408Z",
     "iopub.status.busy": "2025-10-14T13:14:08.533199Z",
     "iopub.status.idle": "2025-10-14T13:14:12.634138Z",
     "shell.execute_reply": "2025-10-14T13:14:12.633546Z",
     "shell.execute_reply.started": "2025-10-14T13:14:08.533392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "df['english'] = df['translation'].apply(lambda x: x['en'])\n",
    "df['french'] = df['translation'].apply(lambda x: x['fr'])\n",
    "\n",
    "df2 = df[['english', 'french']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:14:12.635289Z",
     "iopub.status.busy": "2025-10-14T13:14:12.635080Z",
     "iopub.status.idle": "2025-10-14T13:14:12.653271Z",
     "shell.execute_reply": "2025-10-14T13:14:12.652579Z",
     "shell.execute_reply.started": "2025-10-14T13:14:12.635274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>translation</th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}</td>\n",
       "      <td>The Wanderer</td>\n",
       "      <td>Le grand Meaulnes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}</td>\n",
       "      <td>Alain-Fournier</td>\n",
       "      <td>Alain-Fournier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'en': 'First Part', 'fr': 'PREMIÈRE PARTIE'}</td>\n",
       "      <td>First Part</td>\n",
       "      <td>PREMIÈRE PARTIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'en': 'I', 'fr': 'CHAPITRE PREMIER'}</td>\n",
       "      <td>I</td>\n",
       "      <td>CHAPITRE PREMIER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'en': 'THE BOARDER', 'fr': 'LE PENSIONNAIRE'}</td>\n",
       "      <td>THE BOARDER</td>\n",
       "      <td>LE PENSIONNAIRE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        translation         english  \\\n",
       "0  0  {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}    The Wanderer   \n",
       "1  1   {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}  Alain-Fournier   \n",
       "2  2      {'en': 'First Part', 'fr': 'PREMIÈRE PARTIE'}      First Part   \n",
       "3  3              {'en': 'I', 'fr': 'CHAPITRE PREMIER'}               I   \n",
       "4  4     {'en': 'THE BOARDER', 'fr': 'LE PENSIONNAIRE'}     THE BOARDER   \n",
       "\n",
       "              french  \n",
       "0  Le grand Meaulnes  \n",
       "1     Alain-Fournier  \n",
       "2    PREMIÈRE PARTIE  \n",
       "3   CHAPITRE PREMIER  \n",
       "4    LE PENSIONNAIRE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:14:12.654219Z",
     "iopub.status.busy": "2025-10-14T13:14:12.654029Z",
     "iopub.status.idle": "2025-10-14T13:24:08.932412Z",
     "shell.execute_reply": "2025-10-14T13:24:08.931603Z",
     "shell.execute_reply.started": "2025-10-14T13:14:12.654204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127085/127085 [04:43<00:00, 448.93it/s]\n",
      "100%|██████████| 127085/127085 [05:07<00:00, 413.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---------------\n",
    "# Lower casing words\n",
    "# ---------------\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm', disable = [\"parser\"]) # \"disable = 'parser'\" means disabling parsing. Parsing creates unnecessary tree-based relations for each word to identify whether that particular word is an adjective, noun or pronoun. So, it not necessary at all and disabling it fastens the process.\n",
    "nlp_fr = spacy.load('fr_core_news_sm', disable = [\"parser\"]) # This is for french grammar.\n",
    "\n",
    "def smart_case(sentences, nlp, batch_size = 1000): # This is an advanced smart_case which is built in order to smartly lower-case the required sentence using batches because, the lower casing may take a lot of time. Also, this will take all the sentences at once.\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    # nlp.pipe automatically batches sentences → much faster\n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size = batch_size, n_process = -1), total = len(sentences)): # 'n_process = -1' will use all the CPU cores for processing this. 'tqdm' will show real time processing updates. The 'doc' is a the entire single sentence.\n",
    "        tokens = [] # Here, separate list of tokens will be created for each sentence.\n",
    "        for token in doc: # 'token' captures each word of that sentence.\n",
    "            if token.ent_type_ or token.pos_ in ['PROPN'] or token.text.isupper():\n",
    "                tokens.append(token.text)\n",
    "\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "\n",
    "        processed.append(' '.join(tokens)) \n",
    "\n",
    "    return processed\n",
    "        \n",
    "english_sentences = smart_case(df['english'].tolist(), nlp_en)\n",
    "french_sentences = smart_case(df['french'].tolist(), nlp_fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:44:46.492152Z",
     "iopub.status.busy": "2025-10-15T07:44:46.491593Z",
     "iopub.status.idle": "2025-10-15T07:44:47.551890Z",
     "shell.execute_reply": "2025-10-15T07:44:47.551315Z",
     "shell.execute_reply.started": "2025-10-15T07:44:46.492130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"english_sentences.json\", 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(english_sentences, f, ensure_ascii = False, indent = 2) # ensure_ascii  = False will prevent to treat non-ascii values to something else - saving the text as it is.\n",
    "\n",
    "# with open(\"french_sentences.json\", 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(french_sentences, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/english-french-lower-cased/english_sentences.json\", 'r', encoding = 'utf-8') as f:\n",
    "    english_sentences = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/english-french-lower-cased/french_sentences.json\", 'r', encoding = 'utf-8') as f:\n",
    "    french_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:44:52.409814Z",
     "iopub.status.busy": "2025-10-15T07:44:52.409480Z",
     "iopub.status.idle": "2025-10-15T07:44:52.413989Z",
     "shell.execute_reply": "2025-10-15T07:44:52.413283Z",
     "shell.execute_reply.started": "2025-10-15T07:44:52.409793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the wanderer', 'Alain - Fournier', 'First Part', 'I', 'THE BOARDER']\n",
      "['le grand Meaulnes', 'Alain - Fournier', 'PREMIÈRE PARTIE', 'CHAPITRE PREMIER', 'LE PENSIONNAIRE']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences[:5])\n",
    "print(french_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:44:58.112882Z",
     "iopub.status.busy": "2025-10-15T07:44:58.112310Z",
     "iopub.status.idle": "2025-10-15T07:44:58.194140Z",
     "shell.execute_reply": "2025-10-15T07:44:58.193427Z",
     "shell.execute_reply.started": "2025-10-15T07:44:58.112847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> le grand Meaulnes', '<start> Alain - Fournier', '<start> PREMIÈRE PARTIE', '<start> CHAPITRE PREMIER', '<start> LE PENSIONNAIRE']\n",
      "['le grand Meaulnes <end>', 'Alain - Fournier <end>', 'PREMIÈRE PARTIE <end>', 'CHAPITRE PREMIER <end>', 'LE PENSIONNAIRE <end>']\n"
     ]
    }
   ],
   "source": [
    "french_sentences_input = ['<start> ' + s for s in french_sentences] # This creates a pattern for model saying that after '<start>' the translation should begin and after '<end>' translation should stop for every sentence in decoders.\n",
    "french_sentences_output = [s + ' <end>' for s in french_sentences]\n",
    "english_sentences_input = [s for s in english_sentences]\n",
    "\n",
    "print(french_sentences_input[:5])\n",
    "print(french_sentences_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:24:09.044302Z",
     "iopub.status.busy": "2025-10-14T13:24:09.044038Z",
     "iopub.status.idle": "2025-10-14T13:24:09.875259Z",
     "shell.execute_reply": "2025-10-14T13:24:09.874584Z",
     "shell.execute_reply.started": "2025-10-14T13:24:09.044284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 3256727, ',': 223733, '.': 124433, '\"': 72898, \"'\": 29725, ';': 27424, '-': 26275, 'é': 19581, '’': 17346, '!': 15760, '?': 13344, ':': 7508, 'à': 4937, '−': 2686, 'è': 2591, '_': 2043, 'ê': 1990, '0': 1847, '1': 1532, '—': 1408, '–': 1110, 'â': 887, 'ç': 852, '(': 839, ')': 839, '2': 836, '«': 801, 'ô': 745, '»': 652, '5': 581, '3': 571, 'û': 545, 'É': 543, 'î': 530, '~': 512, '8': 481, '6': 476, '4': 459, '7': 396, '“': 367, '”': 366, '*': 364, 'ù': 359, 'œ': 320, '…': 300, '9': 253, '#': 142, 'æ': 68, 'ï': 67, '°': 56, '&': 54, 'ä': 45, '[': 34, '‘': 33, ']': 32, '`': 24, 'ö': 16, 'ë': 13, '$': 10, '>': 8, 'α': 8, '£': 7, 'ο': 7, 'ü': 5, 'γ': 5, 'ν': 5, 'τ': 5, '/': 4, 'ã': 4, 'Ç': 3, 'А': 3, 'Г': 3, 'Н': 3, 'ί': 3, 'κ': 3, '%': 3, 'Æ': 3, 'έ': 2, 'β': 2, '=': 2, '{': 1, '}': 1, 'ι': 1, 'λ': 1, 'μ': 1, 'ό': 1, 'φ': 1, 'а': 1, 'ε': 1, 'ϊ': 1, 'Œ': 1, '+': 1})\n"
     ]
    }
   ],
   "source": [
    "# -------Checking important symbols in the English sentences by finding all non-alphabet characters-------\n",
    "import re\n",
    "\n",
    "all_text = ' '.join(english_sentences_input) \n",
    "\n",
    "\n",
    "special_characters = re.findall('[^a-zA-Z]', all_text)\n",
    "\n",
    "# This will count frequency of each symbol\n",
    "freq_counter = Counter(special_characters)\n",
    "\n",
    "print(freq_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:45:19.063722Z",
     "iopub.status.busy": "2025-10-15T07:45:19.063095Z",
     "iopub.status.idle": "2025-10-15T07:45:19.103081Z",
     "shell.execute_reply": "2025-10-15T07:45:19.102237Z",
     "shell.execute_reply.started": "2025-10-15T07:45:19.063702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "# ------ Finding Max Length based on the 90 percentile ranking -------\n",
    "\n",
    "eng_lengths = [len(s) for s in english_sentences_input]\n",
    "eng_max_len = int(np.percentile(eng_lengths, 90))\n",
    "print(eng_max_len)\n",
    "\n",
    "fr_lengths = [len(s) for s in french_sentences_input]\n",
    "fr_max_len = int(np.percentile(fr_lengths, 90))\n",
    "print(fr_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Subword Tokenization\n",
    "# ------------------\n",
    "# Instead of Tokenization which leads to OOM problem, I am using subword tokenization which reduces the vocabulary size. E.g., if 'Playful' is a rare word and 'Play' is a common word then, the sentencepiece will convert 'Playful' word into sub words: 'Play' & 'ful', now both the words share a common vocabulary 'Play'. Since we know that in our eng_vocab we have 70k words and there can be many rare words in the vocabulary, so, if we do the subword tokenization for those thousands of words which are rare then, this may decrease the a large amount of vocab size which further prevents OOM problem as this prevents the creation units for uncommon-vocabularies in dense layer, as parameter in dense usually increases if no. of units or vocab_size is larges as parameter_size in Dense = units x vocab_size.\n",
    "\n",
    "with open(\"eng.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in english_sentences:\n",
    "        f.write(s + \"\\n\") # This will make the entire sentences joined with but with '\\n'(because, sentence piece wants each sentence to be like that), which will join all the sentences into a single string.\n",
    "\n",
    "with open(\"fr.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in french_sentences_input + french_sentences_output:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "spm.SentencePieceTrainer.Train(input = 'eng.txt',# 'input' will save each sentence for training.\n",
    "                               model_prefix = 'eng_sp', vocab_size = 8000, model_type = 'bpe') # 'model_prefix = 'eng_sp'('english_sentencepiece') is the name of which will be given to our eng_sen after training. 'model_type = bpe' will use the byte pair encoding(e.g., 'Playing' becomes 'Play', 'ing' on the basis of frequency). vocab_size = 8000-12000 is mostly used for sentences in range(120k-500k)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(input = 'fr.txt',\n",
    "                              model_prefix = 'fr_sp', vocab_size = 8000, model_type = 'bpe' )\n",
    "\n",
    "\n",
    "sp_eng = spm.SentencePieceProcessor()\n",
    "sp_eng.Load(\"/kaggle/input/sentence-piece-models/eng_sp.model\")\n",
    "\n",
    "sp_fr = spm.SentencePieceProcessor()\n",
    "sp_fr.Load(\"/kaggle/input/sentence-piece-models/fr_sp.model\")\n",
    "\n",
    "eng_seq = [sp_eng.EncodeAsIds(s) for s in english_sentences_input]\n",
    "fr_seq_in = [sp_fr.EncodeAsIds(s) for s in french_sentences_input]\n",
    "fr_seq_out = [sp_fr.EncodeAsIds(s) for s in french_sentences_output]\n",
    "\n",
    "eng_seq = pad_sequences(eng_seq, maxlen = eng_max_len, padding = 'post') # Given 'maxlen' to avoid overfitting\n",
    "\n",
    "\n",
    "fr_seq_in = pad_sequences(fr_seq_in, maxlen = fr_max_len, padding = 'post')\n",
    "fr_seq_out = pad_sequences(fr_seq_out, maxlen = fr_max_len, padding = 'post')\n",
    "\n",
    "\n",
    "# Vocabulary size\n",
    "eng_vocab = (sp_eng.GetPieceSize()) + 1\n",
    "fr_vocab = (sp_fr.GetPieceSize()) + 1\n",
    "eng_input_len = eng_seq.shape[1]\n",
    "fr_input_len = fr_seq_in.shape[1]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Building Encoder-Decoder model\n",
    "# --------------------------\n",
    "embedding_dim = 128 # change later if it underfits\n",
    "units =  256 # 256-338 is enough for 8k vocab size\n",
    "drop = 0.3\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape = (eng_input_len, ))  # This input is written has a shape of (max_eng_len, ) instead of (1, max_eng_len) because, doing (1, max_eng_len)  will make each word as a feature of a sentence which the model doesn't expect instead, since we know that model train all the sentences on the basis of batch size, so, we do (max_eng_len, ) in order to state that \"Hey model take this input_len which is the maximum input len of all the english sentence. Later, you'll add a batch size to it in order to train sentence at each time_step\".\n",
    "encoder_emb = Embedding(eng_vocab,embedding_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_emb = Dropout(drop)(encoder_emb)\n",
    "encoder_lstm = LSTM(units, return_state = True, dropout = drop, recurrent_dropout = drop)\n",
    "_, state_h, state_c = encoder_lstm(encoder_emb)\n",
    "encoder_state = [state_h, state_c] # Both states needed\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape = (fr_input_len, ))\n",
    "decoder_emb = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_inputs) # 'mask_zero = True' tells the keras that ignore the timesteps which are '0'. Embeddding will not happen to those timesteps which have '0' values as well as model will ignore them due to paddings. Always give this parameter.\n",
    "decoder_emb = Dropout(drop)(decoder_emb)\n",
    "decoder_lstm = LSTM(units, return_sequences = True, return_state = True, dropout = drop, recurrent_dropout = drop)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state = encoder_state)\n",
    "decoder_dense = Dense(fr_vocab, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs) # Capturing the final output of the decoder. The ouput returns the probability vector for each timestep. Shape = (batch_size, timestep, units)\n",
    "\n",
    "\n",
    "# Seq2Seq Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # Now, creating a model that will act as a pipeline which takes both 'Encdoer' and 'Decoder' inputs-outpus and convert encoder-decoder into a single trainable unit.\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Training Model\n",
    "#--------------\n",
    "model.fit([eng_seq, fr_seq_in], np.expand_dims(fr_seq_out, -1), # This is called teacher forcing — we teach the model about what the correct “previous words” were at each timestep, and that is the reason why we are giving 'fr_in_seq' along with 'en_seq'. We did ' np.expand_dims(fr_out_seq, -1)' because as we know that (n, -1) will make a 2D vector('2D vector' because, earlier 'fr_out_seq' was in the shape of (n, ) which is a list like shape and which is a 1D vector). sparse_categorical_crossentropy needs an extra feature dimension so provide it by using '-1' which means - fr_out_seq shape before expand_dims: (num_samples, max_fr_len) → 2D, After np.expand_dims(..., -1): (num_samples, max_fr_len, 1) → 3D. Now Keras can match it with the model output shape (num_samples, max_fr_len, vocab_fr).\n",
    "            batch_size = 16, epochs = 200, verbose = 2, validation_split = 0.1, callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)])\n",
    "print(\"Training done!\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Building Inference Models: # This Inference phase → uses the learned patterns to generate predictions one token at a time. Encoder inference model — gives us the encoder state for the input sentence. Decoder inference model — takes one word at a time and predicts the next word. The Inference phase is a testing phase.\n",
    "# -----------------------\n",
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_inputs, encoder_state) # It takes encoder inputs which is the input(test) sentence and the 'encoder_state' as an output context, and later, this context vector will be predicted by the model during translation.\n",
    "\n",
    "\n",
    "# Decoder Inference Model\n",
    "decoder_state_input_h = Input(shape = (units, )) # This creates a placeholder for the decoder’s previous hidden state(you just try to understand the below decoder_model's prediction working in order to understand it). Remember: during translation, at each loop iteration, we’ll feed the decoder the last predicted hidden state. So this decoder_state_input acts like: “Hey decoder, here’s where you were in the previous word.”\n",
    "decoder_state_input_c = Input(shape = (units, )) \n",
    "dec_emb2 = Embedding(fr_vocab, embedding_dim,  mask_zero = True)(decoder_inputs) # We are sending 'decoder_inputs' as input shape even though later on, the input will be a single word because, in the 'Embedding', the size of the input is not rigid to the specified size to the 'Embedding'(like decoder_inputs). Also, the 'Embedding' layer only wants a condition to be true while handling that single input is that the input must be having the specified embedding dimensions(like embedding_dim which we already gave, so everything is fine).\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = [decoder_state_input_h, decoder_state_input_c])  # 'initial_state=[decoder_state_input_h, decoder_state_input_c])' → this tells LSTM: “Start processing this word from where you left off last time.”\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # This is the final Dense layer (softmax over vocabulary). It converts the GRU output (hidden vector) into vocabulary probabilities.So if vocab_fr = 5000, then this gives a vector like: [0.01, 0.05, 0.70, 0.02, ...] meaning --> “70% chance the next word is token 3.”\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h, decoder_state_input_c], # Inputs: [decoder_inputs, decoder_state_input_h, decoder_state_input_c] → The model will take both the current token and the previous hidden and cell states. \n",
    "                     [decoder_outputs2, state_h2, state_c2]) # Outputs: [decoder_outputs2, state_h2, state_c2] → It will return the predicted next-token probabilities and the updated hidden and cell states.\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Translation(Prediction): # The working of this is written in the markdown notes.\n",
    "# -----------------------\n",
    "reverse_fr_index = {i: w for w, i in fr_tokenizer.word_index.items()} # This will form a dictionary - used later.\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    sentence = ' '.join(smart_case([sentence], nlp_en))\n",
    "    seq = eng_spm.EncodeAsIds(sentence)\n",
    "    seq = pad_sequences([seq], maxlen = eng_max_len, padding = 'post') # Since we pass lists of tokenized sentences to pad_sequences(see the above eng_seq output).\n",
    "    state = encoder_model.predict(seq) # Because encoder_model returns two states\n",
    "\n",
    "    decoded_sentence = []\n",
    "    start_id = fr_spm.PieceToId('<start>')\n",
    "    input_id = np.array([[start_id]])\n",
    "    \n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        output_, output_h, output_c = decoder_model.predict([input_id] + state)\n",
    "        output_id = np.argmax(output_[0, -1, :])\n",
    "        word = fr_spm.DecodeIds([output_id])\n",
    "\n",
    "        if word == '<end>' or len(decoded_sentence) > fr_max_len:\n",
    "            stop = True\n",
    "\n",
    "        else:\n",
    "            decoded_sentence.append(word)\n",
    "            input_id = np.array([[output_id]])\n",
    "            state = [output_h, output_c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T13:24:09.973936Z",
     "iopub.status.idle": "2025-10-14T13:24:09.974220Z",
     "shell.execute_reply": "2025-10-14T13:24:09.974055Z",
     "shell.execute_reply.started": "2025-10-14T13:24:09.974046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Testing-Phase\n",
    "# ----------\n",
    "sentences = [\"I love eating apples.\", \"Please forgive me.\", \"Stay alert!\"]\n",
    "predictions = []\n",
    "\n",
    "for s in sentences:\n",
    "    predictions.append(sentence_translation(s))\n",
    "    print(s)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation-Phase\n",
    "# ----------------------\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "scores = []\n",
    "references = [\"J'adore manger des pommes.\",\"Pardonnez-moi, s'il vous plaît.\", \"Restez vigilant(e) !\"]\n",
    "\n",
    "for ref, pred in zip(references, predictions):\n",
    "    score = sentence_bleu([ref.split()], [pred.split()])\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average Bleu Score: {sum(scores) / len(scores)}.:4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# </br> Using Bidirectional LSTM without attention. Note, model is not trained because of large sequences and when I drop some sequences then the accuracy decreases which we do not want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:21:21.564163Z",
     "iopub.status.busy": "2025-10-16T06:21:21.563895Z",
     "iopub.status.idle": "2025-10-16T06:21:46.572288Z",
     "shell.execute_reply": "2025-10-16T06:21:46.571454Z",
     "shell.execute_reply.started": "2025-10-16T06:21:21.564140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 06:21:23.201228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760595683.394976      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760595683.450514      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr_core_news_sm\n",
      "  Downloading fr_core_news_sm-3.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr_core_news_sm\n",
      "Successfully installed fr_core_news_sm-3.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Input, Embedding, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "!pip install fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:22:17.856808Z",
     "iopub.status.busy": "2025-10-16T06:22:17.855262Z",
     "iopub.status.idle": "2025-10-16T06:22:22.430431Z",
     "shell.execute_reply": "2025-10-16T06:22:22.429862Z",
     "shell.execute_reply.started": "2025-10-16T06:22:17.856774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e05eb662cc44b728df9024d16e46fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a598b50f269844919399cbcc13782fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-fr/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234f651e78df4541bd2f9eab1fbccccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 127085\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"opus_books\", \"en-fr\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:22:26.271807Z",
     "iopub.status.busy": "2025-10-16T06:22:26.271509Z",
     "iopub.status.idle": "2025-10-16T06:22:30.306630Z",
     "shell.execute_reply": "2025-10-16T06:22:30.306043Z",
     "shell.execute_reply.started": "2025-10-16T06:22:26.271786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['train'])\n",
    "df['english'] = df['translation'].apply(lambda x: x['en'])\n",
    "df['french'] = df['translation'].apply(lambda x: x['fr'])\n",
    "\n",
    "df2 = df[['english', 'french']]\n",
    "\n",
    "df3 = df2.head(30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:22:33.857186Z",
     "iopub.status.busy": "2025-10-16T05:22:33.856870Z",
     "iopub.status.idle": "2025-10-16T05:25:23.179758Z",
     "shell.execute_reply": "2025-10-16T05:25:23.178828Z",
     "shell.execute_reply.started": "2025-10-16T05:22:33.857162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [01:19<00:00, 378.57it/s]\n",
      "100%|██████████| 30000/30000 [01:27<00:00, 341.61it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm', disable = ['parser'])\n",
    "nlp_fr = spacy.load('fr_core_news_sm', disable = ['parser'])\n",
    "\n",
    "def smart_case(sentences, nlp, batch_size = 1000):\n",
    "    processed = []\n",
    "    \n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size = batch_size, n_process = -1), total = len(sentences)):\n",
    "        tokens = []\n",
    "\n",
    "        for token in doc:\n",
    "            \n",
    "            \n",
    "            if token.ent_type_ or token.pos_ in ['PROPN'] or token.text.isupper():\n",
    "                tokens.append(token.text)\n",
    "\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "\n",
    "        processed.append(' '.join(tokens))\n",
    "        \n",
    "    return processed\n",
    "\n",
    "english_sentences = smart_case(df3['english'].tolist(), nlp_en)\n",
    "french_sentences = smart_case(df3['french'].tolist(), nlp_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:23:33.766993Z",
     "iopub.status.busy": "2025-10-16T06:23:33.766394Z",
     "iopub.status.idle": "2025-10-16T06:23:33.986391Z",
     "shell.execute_reply": "2025-10-16T06:23:33.985550Z",
     "shell.execute_reply.started": "2025-10-16T06:23:33.766969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"english_smartCased_sentences.json\", \"w\", encoding = 'utf-8') as f:\n",
    "#     json.dump(english_sentences, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "# with open(\"french_smartCased_sentences.json\", \"w\", encoding = 'utf-8') as f:\n",
    "#     json.dump(french_sentences, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "with open(\"/kaggle/input/smart-cased-sentences/english_smartCased_sentences.json\", \"r\", encoding = 'utf-8') as f:\n",
    "    english_sentences = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/smart-cased-sentences/french_smartCased_sentences.json\", \"r\", encoding = 'utf-8') as f:\n",
    "    french_sentences = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(english_senteces[:5])\n",
    "print(french_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:23:47.588982Z",
     "iopub.status.busy": "2025-10-16T06:23:47.588291Z",
     "iopub.status.idle": "2025-10-16T06:23:47.611325Z",
     "shell.execute_reply": "2025-10-16T06:23:47.610598Z",
     "shell.execute_reply.started": "2025-10-16T06:23:47.588955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "french_input_sentences = ['<start>' + s for s in french_sentences]\n",
    "french_output_sentences = [s + '<end>' for s in french_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"en.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for s in english_sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "with open(\"fr.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for s in french_sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(input = \"en.txt\", vocab_size = 3000, model_type = \"bpe\", model_prefix = \"eng_sp\")\n",
    "spm.SentencePieceTrainer.Train(input = \"fr.txt\", vocab_size = 3000, model_type = \"bpe\", model_prefix = \"fr_sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:24:10.154403Z",
     "iopub.status.busy": "2025-10-16T06:24:10.153847Z",
     "iopub.status.idle": "2025-10-16T06:24:17.554206Z",
     "shell.execute_reply": "2025-10-16T06:24:17.553610Z",
     "shell.execute_reply.started": "2025-10-16T06:24:10.154380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_spm = spm.SentencePieceProcessor()\n",
    "eng_spm.load(\"eng_sp.model\")\n",
    "\n",
    "fr_spm = spm.SentencePieceProcessor()\n",
    "fr_spm.load(\"fr_sp.model\")\n",
    "\n",
    "eng_seq = [eng_spm.EncodeAsIds(s) for s in english_sentences]\n",
    "fr_seq_in = [fr_spm.EncodeAsIds(s) for s in french_input_sentences]\n",
    "fr_seq_out = [fr_spm.EncodeAsIds(s) for s in french_output_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T06:24:28.662515Z",
     "iopub.status.busy": "2025-10-16T06:24:28.661969Z",
     "iopub.status.idle": "2025-10-16T06:24:28.676720Z",
     "shell.execute_reply": "2025-10-16T06:24:28.676190Z",
     "shell.execute_reply.started": "2025-10-16T06:24:28.662491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in english_sentences]\n",
    "eng_max_len = int(np.percentile(lengths, 90))\n",
    "\n",
    "lengths2 = [len(s) for s in french_sentences]\n",
    "fr_max_len = int(np.percentile(lengths2, 90))\n",
    "\n",
    "eng_vocab = eng_spm.GetPieceSize() + 1\n",
    "fr_vocab = fr_spm.GetPieceSize() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "units = 128\n",
    "dropout = 0.2\n",
    "\n",
    "eng_seq = pad_sequences(eng_seq, padding = 'post', maxlen = eng_max_len)\n",
    "fr_seq_in = pad_sequences(fr_seq_in, padding = 'post', maxlen = fr_max_len)\n",
    "fr_seq_out = pad_sequences(fr_seq_out, padding = 'post', maxlen = fr_max_len)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape = (eng_max_len,))\n",
    "encoder_emb = Embedding(eng_vocab, embedding_dim, mask_zero = True)(encoder_input)\n",
    "encoder_emb = Dropout(dropout)(encoder_emb)\n",
    "encoder_biLstm = Bidirectional(LSTM(units, return_state = True, dropout = dropout, recurrent_dropout = dropout))\n",
    "encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_biLstm(encoder_emb)\n",
    "combined_h = Concatenate()([forward_h, backward_h])\n",
    "combined_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape = (fr_max_len, ))\n",
    "decoder_emb = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input)\n",
    "decoder_emb = Dropout(dropout)(decoder_emb)\n",
    "\n",
    "decoder_lstm = LSTM(units*2, return_sequences = True, return_state = True, dropout = dropout)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_emb, initial_state = [combined_h, combined_c])\n",
    "decoder_dense = Dense(fr_vocab, activation = \"softmax\")\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "model = Model([encoder_input, decoder_input],decoder_output)\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit([eng_seq, fr_seq_in], fr_seq_out, validation_split = 0.1, epochs = 200, batch_size = 512, callbacks = [EarlyStopping(monitor = 'val_loss', patience = 8, restore_best_weights = True)])\n",
    "print(model.summary())\n",
    "\n",
    "# Seq2Seq Model\n",
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_input, [combined_h, combined_c]) # This is the encoder inference model.\n",
    "\n",
    "\n",
    "# Decoder Inference Model\n",
    "decoded_input = Input(shape = (1, ))\n",
    "decoder_input_h = Input(shape = (units*2, ))\n",
    "decoder_input_c = Input(shape = (units*2, ))\n",
    "\n",
    "decoder_emb2 = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input)\n",
    "decoder_outputs2, decoder_output_h, decoder_output_c =  decoder_lstm(decoder_emb2, initial_state = [decoder_input_h, decoder_input_c])\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model([decoder_input, decoder_input_h, decoder_input_c],\n",
    "                      [decoder_outputs2, decoder_output_h, decoder_output_c])\n",
    "\n",
    "# Translation:\n",
    "def sentence_translation(sentence):\n",
    "    sentence = ' '.join(smart_case([sentence], nlp_en))\n",
    "    seq = eng_spm.EncodeAsIds(sentence)\n",
    "    seq = pad_sequences([seq], maxlen = eng_max_len, padding = 'post') # Since we pass lists of tokenized sentences to pad_sequences(see the above eng_seq output).\n",
    "    state_h, state_c = encoder_model.predict(seq) # Because encoder_model returns two states\n",
    "\n",
    "    decoded_sentence = []\n",
    "    start_id = fr_spm.PieceToId('<start>')\n",
    "    input_id = np.array([[start_id]])\n",
    "    \n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        output_, output_h, output_c = decoder_model.predict([input_id, state_h, state_c])\n",
    "        output_id = np.argmax(output_[0, -1, :])\n",
    "        word = fr_spm.DecodeIds([output_id])\n",
    "\n",
    "        if word == '<end>' or len(decoded_sentence) > fr_max_len:\n",
    "            stop = True\n",
    "\n",
    "        else:\n",
    "            decoded_sentence.append(word)\n",
    "            input_id = np.array([[output_id]])\n",
    "            state_h, state_c = output_h, output_c\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Testing-Phase\n",
    "# ----------\n",
    "sentences = [\"I love eating apples.\", \"Please forgive me.\", \"Stay alert!\"]\n",
    "predictions = []\n",
    "\n",
    "for s in sentences:\n",
    "    predictions.append(sentence_translation(s))\n",
    "    print(s)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation-Phase\n",
    "# ----------------------\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "scores = []\n",
    "references = [\"J'adore manger des pommes.\",\"Pardonnez-moi, s'il vous plaît.\", \"Restez vigilant(e) !\"]\n",
    "\n",
    "for ref, pred in zip(references, predictions):\n",
    "    score = sentence_bleu([ref.split()], [pred.split()])\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average Bleu Score: {sum(scores) / len(scores)}.:4\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8504182,
     "sourceId": 13400890,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
