# ğğ®ğ¢ğ¥ğğ¢ğ§ğ  ğ„ğ§ğœğ¨ğğğ«â€“ğƒğğœğ¨ğğğ« ğŒğ¨ğğğ¥ğ¬ ğ°ğ¢ğ­ğ¡ ğ‹ğ’ğ“ğŒ & ğğ¢ğ‹ğ’ğ“ğŒ ğŸğ¨ğ« ğ’ğğªğ®ğğ§ğœğ-ğ­ğ¨-ğ’ğğªğ®ğğ§ğœğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ 

From past four days, I have been implementing an Encoderâ€“Decoder architecture from scratch â€” using **BiLSTM and LSTM** â€” to understand how modern translation models process sequences.  
I already knew how to implement the **RNNs (LSTM, GRU, BiLSTM)**.

---

## â€¢  Steps I Followed

### **1. Data Preprocessing**
- Lowercased all sentences for uniformity.  
- Used `nlp.pipe` for efficient sentence batching.  
- Saved Englishâ€“French datasets as `.json` with proper UTF-8 encoding (`ensure_ascii=False`).

---

### **2. Symbol & Length Analysis**
- Checked non-alphabetic symbols across the corpus.  
- Computed maximum sentence length using the **90th percentile** â€” a balance between memory and coverage.

---

### **3. Subword Tokenization**
- Instead of traditional tokenization (which causes OOM issues with large vocabularies), used **SentencePiece** for subword-level tokenization.  
- Example: *â€œPlayfulâ€ â†’ â€œPlayâ€ + â€œfulâ€*  
- This significantly reduced vocabulary size and memory footprint during training.

---

### **4. Building the Model**
- Designed an **Encoderâ€“Decoder (Seq2Seq)** model using **LSTM/BiLSTM** layers.  
- Added **attention-ready architecture** for future expansion.  
- Trained the model on preprocessed Englishâ€“French pairs.

---

### **5. Inference Phase**
- Built a separate **inference model** to generate translations token-by-token.  
- The encoder provides hidden states, which the decoder uses iteratively to predict the next token.

---

## â€¢ Learnings

1. **Subword tokenization** prevents memory overflow and improves vocabulary efficiency.  
2. **LSTM and BiLSTM** capture contextual dependencies effectively in both directions.  
3. Designing **separate inference pipelines** is crucial for real-world translation tasks.  
4. **90th percentile max-length selection** keeps training efficient without major data loss.

---

ğŸ”— **LinkedIn:** [https://linkedin.com/tajwinder-singh-](https://www.linkedin.com/in/tajwinder-singh-)
---

#DeepLearning #NLP #Seq2Seq #LSTM #BiLSTM #EncoderDecoder #MachineLearning #LanguageTranslation #AI #DataScience #NeuralNetworks
<img width="1365" height="629" alt="image" src="https://github.com/user-attachments/assets/b7845ac9-5a4f-4391-ba92-a3884445ac97" />
